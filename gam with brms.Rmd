---
title: "Bayesian GAMs and non-constant variance"
output: github_document
author: "Bindoff, A."
---

`r Sys.Date()`

The first half of this analysis is lifted from a blog by Gavin Simpson https://www.fromthebottomoftheheap.net/2018/04/21/fitting-gams-with-brms/, which shows how to fit a GAM with the `brms` package and Stan in R. The analysis is well documented in the aforementioned blog, and includes a suggestion that in Part II the non-constant variance will be modelled.  

Inspired by how easy it was to fit the model in `brms` I decided not to wait for Part II, and figured out how to model the non-constant variance myself.  


```{r, echo = F, include = F}
knitr::opts_chunk$set(message = F, warning = F)
## fitting GAMs with `brms`  (example lifted from https://www.fromthebottomoftheheap.net/2018/04/21/fitting-gams-with-brms/)

library('mgcv')
library('brms')
library('ggplot2')
library('schoenberg')
library(bayesplot)

#theme_set(theme_bw())

```

Load the example data mcycle and plot  


```{r}
## load the example data mcycle
data(mcycle, package = 'MASS')

## show data
head(mcycle)
ggplot(mcycle, aes(x = times, y = accel)) +
  geom_point() +
  labs(x = "Miliseconds post impact", y = "Acceleration (g)",
       title = "Simulated Motorcycle Accident",
       subtitle = "Measurements of head acceleration")
```
  
There is no way to fit a straight line to this data, and fitting a polynomial might also present some challenges. Clearly, the assumption of constant variance is violated but for the purposes of exposition we will proceed to fit a GAM and a Bayesian GAM using a smooth spline.  

Fit a model, $m_1$ with a GAM using the `mgcv` package.  


```{r}
m1 <- gam(accel ~ s(times), data = mcycle, method = "REML")
summary(m1)
```

Plot the marginal effects  


```{r}
me <- predict(m1, newdata = mcycle, type = "response", se = T)
me <- data.frame(me, mcycle)

ggplot(me, aes(x = times, y = fit)) +
  geom_line(size = 1, colour = "blue") +
  geom_ribbon(mapping = aes(ymin = fit - 1.96*se.fit, ymax = fit + 1.96*se.fit), alpha = 0.2) +
  geom_point(data = mcycle, aes(x = times, y = accel)) +
  labs(y = "accel")



```
  
Bayesian model, $m_2$, fitted with `brms` as per blog.  


```{r}
m2 <- brm(bf(accel ~ s(times)),
          data = mcycle, family = gaussian(), cores = 2, seed = 17,
          iter = 4000, warmup = 1000, thin = 10, refresh = 0, 
          control = list(adapt_delta = 0.99))

summary(m2)

plot(marginal_effects(m2), points = TRUE)
```

  
  A possible solution is to model the non-constant variance by fitting a distributional regression model, $m_3$ which allows for this. The `bf` function in `brms` makes this trivially easy (in this example, at least) by letting you casually specify the additional term `sigma ~ s(times)` like you were ordering a kebab at 3am.  
  

```{r}
m3 <- brm(bf(accel ~ s(times), sigma ~ s(times)),
  data = mcycle, family = gaussian(),
  cores = 2, seed = 17,
  iter = 4000, warmup = 1000, thin = 10, refresh = 0, 
  control = list(adapt_delta = 0.99, max_treedepth = 15)
)

summary(m3)
plot(marginal_effects(m3), points = TRUE)
```

  
  This looks much better, it's a good kebab at any time of day or night. Now compare models using posterior predictive checks (generate data from the models $y^{rep}$ and compare to observed data $y$).  
  

```{r}
par(mfrow = c(2, 2))
pp_check(m2, nsamples = 30) + ggplot2::scale_color_manual(values = c("black", "indianred")) +
  ggplot2::xlim(-200, 200) + labs(title = "m2")
pp_check(m3, nsamples = 30) + ggplot2::scale_color_manual(values = c("black", "dodgerblue")) +
  ggplot2::xlim(-200, 200) + labs(title = "m3")
pp_check(m2, nsamples = 30, type = "ecdf_overlay") + ggplot2::scale_color_manual(values = c("black", "indianred")) +
  ggplot2::xlim(-200, 200) + labs(title = "m2")
pp_check(m3, nsamples = 30, type = "ecdf_overlay") + ggplot2::scale_color_manual(values = c("black", "dodgerblue"))+   ggplot2::xlim(-200, 200) + labs(title = "m3")
```
  
  
Compare models with Leave One Out cross-validation (smaller is better)  



```{r}
LOO(m2, m3)
```

